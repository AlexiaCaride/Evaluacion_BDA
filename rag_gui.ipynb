{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39948/1689085387.py:80: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(question)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_39948/1689085387.py\", line 80, in load_and_ask\n",
      "    response = qa_chain.run(question)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 606, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/retrieval_qa/base.py\", line 154, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "        input_documents=docs, question=question, callbacks=_run_manager.get_child()\n",
      "    )\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 611, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ~~~~~~~~~~~~~~~~~^\n",
      "        docs, callbacks=_run_manager.get_child(), **other_keys\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        prompts,\n",
      "        ^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        **self.llm_kwargs,\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 755, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 950, in generate\n",
      "    output = self._generate_helper(\n",
      "        prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n",
      "    )\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 792, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 779, in _generate_helper\n",
      "    self._generate(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        prompts,\n",
      "        ^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py\", line 288, in _generate\n",
      "    final_chunk = self._stream_with_aggregation(\n",
      "        prompt,\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py\", line 256, in _stream_with_aggregation\n",
      "    for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py\", line 211, in _create_generate_stream\n",
      "    yield from self._client.generate(\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        **self._generate_params(prompt, stop=stop, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )  # type: ignore\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/ollama/_client.py\", line 167, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model requires more system memory (1.5 GiB) than is available (328.4 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio/flagged/dataset1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_39948/1689085387.py\", line 80, in load_and_ask\n",
      "    response = qa_chain.run(question)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 606, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/retrieval_qa/base.py\", line 154, in _call\n",
      "    answer = self.combine_documents_chain.run(\n",
      "        input_documents=docs, question=question, callbacks=_run_manager.get_child()\n",
      "    )\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 611, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "                                ~~~~~~~~~~~~~~~~~^\n",
      "        docs, callbacks=_run_manager.get_child(), **other_keys\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/combine_documents/stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "           ~~~~~~~~~~~^\n",
      "        inputs,\n",
      "        ^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        include_run_info=include_run_info,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain/chains/llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        prompts,\n",
      "        ^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        **self.llm_kwargs,\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 755, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 950, in generate\n",
      "    output = self._generate_helper(\n",
      "        prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n",
      "    )\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 792, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_core/language_models/llms.py\", line 779, in _generate_helper\n",
      "    self._generate(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        prompts,\n",
      "        ^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py\", line 288, in _generate\n",
      "    final_chunk = self._stream_with_aggregation(\n",
      "        prompt,\n",
      "    ...<3 lines>...\n",
      "        **kwargs,\n",
      "    )\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py\", line 256, in _stream_with_aggregation\n",
      "    for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/langchain_ollama/llms.py\", line 211, in _create_generate_stream\n",
      "    yield from self._client.generate(\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        **self._generate_params(prompt, stop=stop, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )  # type: ignore\n",
      "    ^\n",
      "  File \"/home/alex/miniconda3/envs/rag/lib/python3.13/site-packages/ollama/_client.py\", line 167, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model requires more system memory (1.5 GiB) than is available (313.8 MiB)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate  # Importar PromptTemplate\n",
    "\n",
    "# Función para extraer texto de una URL\n",
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Función para extraer texto de un archivo PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en fragmentos\n",
    "def split_text(text, chunk_size=500):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Cargar el modelo Llama 3.2\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\", server_url=\"http://localhost:11434\")\n",
    "\n",
    "# Configuración de embeddings y vectorstore\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=\"./vectorstore\", embedding_function=embedding_model)\n",
    "\n",
    "# Crear el PromptTemplate\n",
    "prompt_template = \"\"\"\n",
    "Eres un asistente útil. Usa el contexto a continuación para responder la pregunta del usuario:\n",
    "\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Configurar el retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Crear el chain de preguntas y respuestas\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Función que maneja la carga de PDF, URL y la consulta\n",
    "def load_and_ask(pdf_file, url, question):\n",
    "    if not pdf_file and not url:\n",
    "        return \"Por favor, sube un archivo PDF o proporciona una URL.\"\n",
    "\n",
    "    # Si se proporciona un PDF\n",
    "    if pdf_file:\n",
    "        page_content = extract_text_from_pdf(pdf_file.name)\n",
    "    # Si se proporciona una URL\n",
    "    elif url:\n",
    "        page_content = extract_text_from_url(url)\n",
    "    \n",
    "    # Dividir el texto en fragmentos\n",
    "    chunks = split_text(page_content)\n",
    "    \n",
    "    # Convertir los fragmentos en documentos y agregarlos al vectorstore\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    vectorstore.add_documents(documents)\n",
    "    \n",
    "    # Hacer la consulta\n",
    "    if question:\n",
    "        response = qa_chain.run(question)\n",
    "        return response\n",
    "    else:\n",
    "        return \"Por favor ingresa una pregunta.\"\n",
    "\n",
    "# Crear la interfaz con Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=load_and_ask,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Cargar PDF (Español)\"),\n",
    "        gr.Textbox(label=\"Introduce la URL (Inglés)\"),\n",
    "        gr.Textbox(label=\"Escribe tu pregunta\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    live=True\n",
    ")\n",
    "\n",
    "# Iniciar la interfaz\n",
    "iface.launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
